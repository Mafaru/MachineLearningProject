% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Report},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{graphicx}
\usepackage{float}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi


\title{
    \fontsize{28}{32}\selectfont
    \textcolor{teal}{\bfseries Report Machine Learning's Project}
}
\author{Mario Morra 2156770, Leonardo Sereni 1846461}
\date{}

\begin{document}


\maketitle

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{logo.png}
\end{figure}

\vspace{2cm}

\hypertarget{project-objectives}{%
\section{Project Objectives}\label{project-objectives}}

The objective of this project is to design, implement, and analyze
Reinforcement Learning agents based on the Q-learning framework. Two
complementary approaches are considered: a tabular representation of the
action-value function and a function approximation approach based on
Deep Q-Networks (DQN).

The main goals of the project are:

\begin{itemize}
\item
  To formally model the selected environment as a Markov Decision
  Process (MDP).
\item
  To implement the Q-learning algorithm in its tabular form, explicitly
  representing the action-value function.
\item
  To extend the same learning principle to a neural-network-based
  approximation through a Deep Q-Network.
\item
  To design and analyze an exploration strategy based on the
  epsilon-greedy policy.
\item
  To investigate the convergence behavior and learning dynamics of both
  approaches.
\item
  To evaluate and compare the learned policies in terms of performance,
  stability, and generalization capability.
\end{itemize}

The tabular approach provides a clear and interpretable implementation
of Q-learning in environments with finite and relatively small state
spaces, such as Taxi-v3, where the action-value function can be stored
explicitly. In contrast, the Deep Q-Network approach replaces the table
with a parameterized function approximator, enabling scalability to
larger or continuous state spaces.

Together, these two implementations allow a comprehensive analysis of
value-based reinforcement learning, highlighting both the theoretical
foundations and the practical limitations of tabular methods, as well as
the advantages introduced by neural network approximation.

\hypertarget{environment-description}{%
\section{Environment Description}\label{environment-description}}

\hypertarget{environment-taxi-v3}{%
\subsection{Environment: Taxi-v3}\label{environment-taxi-v3}}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{image.png}
    \caption{Taxi-v3 environment visualization}
    \label{fig:taxi-environment}
\end{figure}
The environment selected for this project is Taxi-v3, a classic
benchmark environment provided by the Gymnasium library. It represents a
discrete, fully observable, episodic Markov Decision Process.

In this environment, a taxi agent operates in a 5×5 grid world. The task
consists of:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Navigating to the passenger's location,
\item
  Executing a pickup action,
\item
  Navigating to the specified destination,
\item
  Executing a dropoff action.
\end{enumerate}

The state space consists of 500 discrete states. Each state encodes:

\begin{itemize}
\item
  The taxi position (25 possible grid locations),
\item
  The passenger location (4 fixed locations or inside the taxi),
\item
  The destination location (4 fixed locations).
\end{itemize}

The action space is composed of 6 discrete actions:

\begin{itemize}
\item
  Move south,
\item
  Move north,
\item
  Move east,
\item
  Move west,
\item
  Pickup passenger,
\item
  Dropoff passenger.
\end{itemize}

The reward structure is defined as follows:

\begin{itemize}
\item
  +20 for successfully delivering the passenger,
\item
  $-1$ for each time step (to encourage efficiency),
\item
  $-10$ for illegal pickup or dropoff actions.
\end{itemize}

Each episode terminates either when the passenger is successfully
delivered or when a maximum number of steps (200) is reached. The latter
condition prevents infinite trajectories and ensures bounded returns.

Taxi-v3 is deterministic, meaning that for each state-action pair, the
next state is uniquely determined. This property simplifies the learning
dynamics and makes the environment particularly suitable for analyzing
tabular Q-learning behavior.

\hypertarget{introduction-to-reinforcement-learning}{%
\section{Introduction to reinforcement
learning}\label{introduction-to-reinforcement-learning}}

Reinforcement Learning (RL) is a learning paradigm in which an agent
interacts with an environment in order to maximize cumulative reward
through trial-and-error experience. Unlike supervised learning, where
labeled input-output pairs are provided, in RL the agent must discover
which actions yield the highest long-term benefit by interacting with
the environment and observing feedback in the form of rewards.

The interaction between the agent and the environment is typically
modeled as a Markov Decision Process (MDP). At each discrete time step
\(t\), the agent:

\begin{itemize}
\item
  observes the current state \(s_{t} \in \mathcal{S}\),
\item
  selects an action \(a_{t} \in \mathcal{A}\),
\item
  receives a reward \(r_{t + 1}\),
\item
  transitions to a new state \(s_{t + 1}\).
\end{itemize}

The objective of the agent is to maximize the expected cumulative
discounted reward, also called the return:

\[
V^{\pi}(s) \equiv 
E \left[ \sum_{t=1}^{\infty} \gamma^{t-1} r_t \right]
\]


where \(\left. \ \gamma \in \lbrack 0,1 \right)\) is the discount factor.
The discount factor determines the relative importance of future rewards
compared to immediate rewards. A value of \(\gamma\) close to 1
encourages long-term planning, while a smaller value makes the agent
more short-sighted.

A central concept in Reinforcement Learning is the policy, denoted by
\(\pi\). A policy defines the behavior of the agent and specifies how
actions are chosen given states:

\[
\pi : S \rightarrow A
\]


The goal of learning is to find an optimal policy
\[
\pi^{*} \equiv \arg\max_{\pi} V^{\pi}(s), \quad \forall s \in S
\]
that maximizes the expected cumulative discounted reward.

To evaluate state-action pairs, the action-value function (Q-function) is introduced:

\[
Q^{\pi}(s,a) \equiv r(s,a) + \gamma V^{\pi}(\delta(s,a))
\]

which represents the expected cumulative discounted reward obtained by
executing action $a$ in state $s$ and subsequently following policy $\pi$.

The optimal action-value function \(Q^{*}(s,a)\) satisfies the Bellman
optimality equation:

\[
Q(s,a) = r(s,a) + \gamma \max_{a' \in A} Q(\delta(s,a),a')
\]


This recursive relationship expresses the principle of optimality: the
value of a state-action pair equals the immediate reward plus the
discounted value of the best possible action in the next state.

In practice, the optimal Q-function is not known in advance and must be
approximated through interaction with the environment. Q-learning is one
such method, using a temporal-difference update rule to iteratively
approximate the Bellman optimality equation. In the tabular case, the
action-value function is stored explicitly as a table and updated after
each interaction step.

\hypertarget{from-theory-to-implementation}{%
\section{From Theory to
Implementation}\label{from-theory-to-implementation}}

The Bellman optimality equation provides a theoretical characterization
of the optimal action-value function.
However, this equation alone does not provide a direct computational
method, since the optimal function \(Q^{*}\) is unknown. In practice, we
approximate this function iteratively through interaction with the
environment.

In the tabular setting, the action-value function is represented
explicitly as a matrix of size
\(\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid\). Each row
corresponds to a state, and each column corresponds to a possible
action. The entry \(Q(s,a)\) stores the current estimate of the expected
cumulative discounted reward obtained by taking action \(a\) in state
\(s\)and subsequently acting optimally.

Initially, all Q-values are set to zero, representing a complete lack of
prior knowledge. During training, the agent interacts with the
environment over multiple episodes. At each step, after observing the
reward and the next state, the Q-value corresponding to the executed
state-action pair is updated according to:

\[Q(s,a) \leftarrow Q(s,a) + \alpha\lbrack r + \gamma\underset{a^{'}}{\max}Q(s^{'},a^{'}) - Q(s,a)\rbrack
\]

This update progressively pushes the current estimate toward the value
suggested by the Bellman equation. Over many episodes, information about
successful trajectories propagates backward through the table, allowing
the agent to approximate the optimal policy.

\hypertarget{exploration-strategy}{%
\subsection{Exploration strategy}\label{exploration-strategy}}

Since the Q-values are initially inaccurate, the agent must explore the
environment. For this reason, an \(\epsilon\)-greedy strategy is
adopted:

\[\pi(a \mid s) = \left\{ \begin{matrix}
\text{random\ action} & \text{with\ probability\ }\epsilon \\
\arg{\max}_{a}Q(s,a) & \text{with\ probability\ }1 - \epsilon \\
\end{matrix} \right.\ 
\]

The exploration parameter \(\epsilon\) is progressively reduced during
training in order to shift from exploration to exploitation. This
ensures that early learning phases sufficiently cover the state-action
space, while later phases focus on refining the learned policy.

\hypertarget{training-procedure}{%
\subsection{Training Procedure}\label{training-procedure}}

Training is performed over 10,000 episodes. At the beginning of each
episode, the environment is reset and a new initial state is sampled.
The agent then repeatedly:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Selects an action according to the \(\epsilon\)-greedy policy.
\item
  Executes the action in the environment.
\item
  Observes the reward and the next state.
\item
  Updates the corresponding Q-table entry.
\end{enumerate}

Each episode terminates either when the passenger is successfully
delivered or when the maximum number of steps imposed by the environment
is reached.

\hypertarget{implementation-of-the-tabular-q-learning-agent}{%
\section{Implementation of the Tabular Q-Learning Agent}\label{implementation-of-the-tabular-q-learning-agent}}

The tabular agent is implemented as a Python class that explicitly
stores the action-value function \(Q(s,a)\)as a matrix with shape
\(\mid \mathcal{S} \mid \times \mid \mathcal{A} \mid\). In the Taxi-v3
environment, the state space is discrete with
\(\mid \mathcal{S} \mid = 500\)and the action space has
\(\mid \mathcal{A} \mid = 6\)actions. The Q-table is initialized to
zero, meaning that initially the agent has no preference among actions.

\hypertarget{agent-initialization}{%
\subsection{Agent Initialization}\label{agent-initialization}}

The constructor receives the environment dimensions and learning
hyperparameters. In particular:

\begin{itemize}
\item
  \(\gamma\)(discount factor) controls the importance of future rewards.
\item
  \(\epsilon\) controls exploration through an \(\epsilon\)-greedy
  policy.
\item
  \(\epsilon_{\text{decay}}\) and \(\epsilon_{\min}\) reduce exploration
  over time while keeping a minimum probability of random actions.
\item
  \(\alpha\)(learning rate) controls how strongly new information
  updates the current estimate.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{image2.png}
    \caption{Agent initialization code}
    \label{fig:agent-init}
\end{figure}

\hypertarget{action-selection-epsilon-greedy-policy}{%
\subsection{\texorpdfstring{Action Selection: \(\epsilon\)-Greedy Policy}{Action Selection: \textbackslash epsilon-Greedy Policy}}\label{action-selection-epsilon-greedy-policy}}

The method select\_action(state, training=True) implements the
exploration strategy:

\begin{itemize}
\item
  during training, with probability \(\epsilon\), a random action is
  sampled uniformly from the discrete action set;
\item
  otherwise, the greedy action is selected as the action with maximum
  estimated value in the current state:
\end{itemize}

\[a = \arg\underset{a \in \mathcal{A}}{\max}Q(s,a).
\]

When training=False, exploration is disabled and the policy becomes
purely greedy, which is appropriate for evaluation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{image3.png}
    \caption{Action selection implementation}
    \label{fig:action-selection}
\end{figure}

\hypertarget{q-table-update}{%
\subsection{Q-table Update}\label{q-table-update}}

After executing an action and observing \(\left( s,a,r,s^{'} \right)\),
the Q-table is updated according to the tabular Q-learning rule:

\[Q(s,a) \leftarrow Q(s,a) + \alpha\lbrack r + \gamma\underset{a^{'}}{\max}Q(s^{'},a^{'}) - Q(s,a)\rbrack.
\]

This update moves the current estimate toward the Bellman optimality
target \(r + \gamma{\max}_{a^{'}}Q(s^{'},a^{'})\), progressively
improving the quality of the learned action-value function.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{image4.png}
    \caption{Q-table update implementation}
    \label{fig:q-update}
\end{figure}
\hypertarget{exploration-decay}{%
\subsection{Exploration decay}\label{exploration-decay}}

The method decay\_epsilon() updates the exploration rate after each
episode:

\[\epsilon \leftarrow \max(\epsilon_{\min},\epsilon \cdot \epsilon_{\text{decay}}),
\]

allowing the agent to explore extensively in early episodes and
gradually exploit the learned policy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{image5.png}
    \caption{Epsilon decay implementation}
    \label{fig:epsilon-decay}
\end{figure}
\hypertarget{training-loop}{%
\subsection{Training Loop}\label{training-loop}}

The function train\_agent(env, agent, num\_episodes, print\_interval)
trains the agent for a fixed number of episodes. Each episode starts
with env.reset() and proceeds until either the task is completed
(terminated=True) or a step limit is reached (truncated=True). At each
step:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  an action is selected via select\_action;
\item
  the environment transition is performed using env.step(action);
\item
  the Q-table is updated via update;
\item
  the next state becomes the current state.
\end{enumerate}

The total reward per episode is recorded in rewards\_history, while the
evolution of \(\epsilon\) is stored in epsilon\_history. Periodically, an
average reward over the last episodes is printed to monitor convergence.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{image6.png}
    \caption{Training loop implementation}
    \label{fig:training-loop}
\end{figure}

\hypertarget{evaluation-and-visualization}{%
\subsection{Evaluation and Visualization}\label{evaluation-and-visualization}}

The function evaluate\_agent runs a set of evaluation episodes using the
greedy policy (training=False) to estimate the final performance,
reporting mean/min/max total rewards. The function
plot\_training\_results visualizes learning dynamics by plotting episode
rewards (with moving average smoothing) and the exploration decay curve.
Finally, run\_demo renders a single greedy episode to qualitatively
inspect the learned behavior.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image7.png}
    \caption{Evaluation function implementation}
    \label{fig:evaluate-agent}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image8.png}
    \caption{Plot training results implementation}
    \label{fig:plot-training}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image9.png}
    \caption{Demo execution implementation}
    \label{fig:run-demo}
\end{figure}





\hypertarget{DQN Part}{%
\section{DQN part}\label{DQN part}}

\hypertarget{Brief introduction}{%
\subsection{Brief introduction}\label{Brief introduction}}

While tabular Q-learning represents the action-value function explicitly
as a lookup table, this approach becomes impractical when the state space
grows large or continuous. The memory requirements scale with
$\lvert \mathcal{S} \rvert \times \lvert \mathcal{A} \rvert$, and many
states may never be visited sufficiently often to learn reliable values.
To address these limitations, the Q-function can instead be approximated
using a parameterized model.


Deep Q-Networks (DQN) replace the Q-table with a neural network
$Q(s,a;\theta)$ that receives the state as input and outputs estimated
Q-values for all actions. This enables generalization across similar
states and removes the need to explicitly store values for every
state-action pair. The learning principle remains based on the Bellman
optimality target, but the update is now performed through gradient-based
optimization.

Instead of updating a single table entry after each interaction,
learning is performed through minibatch gradient descent using
transitions stored in a replay buffer. An initial warm-up phase fills
the buffer with random experiences before training begins, and batches
are sampled uniformly during learning to reduce temporal correlations.

For a sampled minibatch of transitions $(s_i,a_i,r_i,s'_i)$, the network
predicts $Q(s_i,a_i;\theta)$, and targets are computed using the
Bellman update:

\[
y_i = r_i + \gamma \max_{a'} Q(s'_i,a';\theta)
\]

The parameters $\theta$ are optimized by minimizing the Mean Squared
Error (MSE) between predictions and targets:

\[
L(\theta) =
\frac{1}{N} \sum_{i=1}^{N}
\left(
y_i - Q(s_i,a_i;\theta)
\right)^2
\]

Compared to tabular Q-learning, this update
performs regression on batches of past experiences rather than direct
assignment to a table entry.

\hypertarget{Exploration strategy}{%
\subsection{Exploration strategy}\label{Exploration strategy}}

As the Q-table, also with DQN has been adopted an $\epsilon$-greedy strategy, with $\epsilon$ progressively reduced during the training in order to shift from exploration to exploitation.


\hypertarget{Warm-up phase}{%
\subsection{Warm-up phase}\label{Warm-up phase}}

Before training begins, the replay buffer is populated with a set of transitions collected through random interaction with the environment.
This warm-up phase ensures that a sufficient number of experiences are available for minibatch sampling once learning starts.
Without this initialization, early gradient updates would rely on very few and highly correlated samples, leading to unstable learning and poor target estimates. 


\hypertarget{Training phase}{%
\subsection{Training phase}\label{Training phase}}

Training is performed on a fixed number of episodes. At the beginning of each episode the environment is reset. The agent repeatedly:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Selects an action according to the \(\epsilon\)-greedy policy.
\item
  Executes the action in the environment.
\item
  Store transition in the replay buffer.
\item
  Train the Q-network.
\end{enumerate}

Each episode terminates either when the passenger is successfully
delivered or when the maximum number of steps imposed by the environment
is reached.


\hypertarget{Implementation of DQN Agent}{%
\section{Implementation of DQN Agent}\label{Implementation of DQN Agent}}

the DQN agent is implemented as a Python class, that explicitly stores the NN and the replay buffer in addition to the configurations parameters. The NN, Replay buffer and Transitions also are implemented as Python classes. 

\hypertarget{Transition Class}{%
\subsection{Transition Class}\label{Transition Class}}

Immutable dataclass storing transition

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Transition_Class.png}
    \caption{Transition Class}
    \label{fig:Transition Class}
\end{figure}


\hypertarget{ReplayBuffer Class}{%
\subsection{ReplayBuffer Class}\label{ReplayBuffer Class}}

This is the auxiliary Class constructed to model the Replay Buffer using deque structure; it is a circular array.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{ReplayBufferClass.png}
    \caption{ReplayBuffer Class}
    \label{fig:ReplayBuffer Class}
\end{figure}


\hypertarget{QNetwork Class}{%
\subsection{QNetwork Class}\label{QNetwork Class}}

This class stores the Neural Network; for a better managment of NN and operation related to it is used the Pytorch library.
The NN has a Depth(Number of hidden layers) 2, while it has a Width of 128. The activation functions are ReLUs.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{QNetworkClass.png}
    \caption{QNetwork Class}
    \label{fig:QNetwork Class}
\end{figure}


\hypertarget{DQN Agent Initialization}{%
\subsection{DQN Agent Initialization}\label{DQN Agent Initialization}}

The constructor receives the environment dimensions and learning
hyperparameters. In particular:

\begin{itemize}
\item
  \(\gamma\)(discount factor) controls the importance of future rewards.
\item
  \(\epsilon\) controls exploration through an \(\epsilon\)-greedy
  policy.
\item
  \(\epsilon_{\text{decay}}\) and \(\epsilon_{\min}\) reduce exploration
  over time while keeping a minimum probability of random actions.
\item
  batch size define the size of the batch for training 
\item
  buffer size defines the size of the replay buffer
\item
  min buffer size is the minimum buffer size to start training phase

\end{itemize}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{DQNAgent_init.png}
    \caption{DQN Agent initialization}
    \label{fig:DQN Agent initialization}
\end{figure}


\hypertarget{OneHot Function}{%
\subsection{OneHot Function}\label{OneHot Function}}
This function transforms the state(an integer) in a vector composed by all zeros and a one in the position of the specific state

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{OneHot.png}
    \caption{One Hot function}
    \label{fig:One Hot function}
\end{figure}


\hypertarget{action-selection-epsilon-greedy-policy}{%
\subsection{\texorpdfstring{Action Selection: \(\epsilon\)-Greedy Policy}{Action Selection: \textbackslash epsilon-Greedy Policy}}\label{action-selection-epsilon-greedy-policy}}

The method select\_action(state, training=True) implements the
exploration strategy:

\begin{itemize}
\item
  during training, with probability \(\epsilon\), a random action is
  sampled uniformly from the discrete action set;
\item
  otherwise, the greedy action is selected as the action with maximum
  estimated value in the current state:
\end{itemize}

\[a = \arg\underset{a \in \mathcal{A}}{\max}Q(s,a).
\]

When training=False, exploration is disabled and the policy becomes
purely greedy, which is appropriate for evaluation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{SelectAction.png}
    \caption{Action selection implementation}
    \label{fig:action-selection}
\end{figure}

























































































\hypertarget{Train Step Function}{%
\subsection{Train Step Function}\label{Train Step Function}}

The train step function performs a single optimization update of the Deep Q-Network. It samples a minibatch from the replay buffer, converts transitions into tensor representations, computes predicted action-values, and constructs targets using the Bellman equation. The mean squared error between predictions and targets is minimized through backpropagation, and model parameters are updated using an optimizer such as Adam. This process iteratively improves the network’s approximation of the optimal Q-function.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{TrainStep.png}
    \caption{Train Step Function}
    \label{fig:Train Step Function}
\end{figure}



\hypertarget{Epsilon decay Function}{%
\subsection{Epsilon decay}\label{epsilon-decay}}

The method decay\_epsilon() updates the exploration rate after each
episode:

\[\epsilon \leftarrow \max(\epsilon_{\min},\epsilon \cdot \epsilon_{\text{decay}}),
\]

allowing the agent to explore extensively in early episodes and
gradually exploit the learned policy.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{EpsilonDecay.png}
    \caption{Epsilon decay Function}
    \label{fig:epsilon-decay}
\end{figure}


\hypertarget{Warm-Up Function}{%
\subsection{Warm-Up Function}\label{Warm-Up Function}}
 While the Replay Buffer does not stores enough transitions, an action in picked randomly, executed and inserted the entire transition in the Replay Buffer.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{Warmup.png}
    \caption{Warm-Up Function}
    \label{fig:Warm-Up Function}
\end{figure}



\hypertarget{DQN Agent Training Loop}{%
\subsection{DQN Agent Training Loop}\label{DQN Agent Training Loop}}
In this function there is an external loop that iterates on the number of episodes; for each episode, there is an internal loop that: select action with epsilon-greedy policy, execute the action, store transition inn replay buffer and every 4 times trains the QNetwork; at the end of each episode the epsilon is updated(decayed) 
The total reward per episode is recorded in rewards\_history, while the
evolution of \(\epsilon\) is stored in epsilon\_history. Periodically, an
average reward over the last episodes is printed to monitor convergence.
\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\linewidth]{TrainAgent.png}
    \caption{DQN Agent Training Loop}
    \label{fig:DQN Agent Training Loop}
\end{figure}

\section{Experimental Evaluation: Taxi-v3}

\subsection{Experimental Setup}


Both agents were trained for 6000 episodes using:

\begin{itemize}
    \item Initial exploration rate: $\epsilon_0 = 1.0$
    \item Minimum exploration rate: $\epsilon_{min} = 0.01$
    \item Exploration decay values: $\epsilon_{decay} \in \{0.999, 0.997, 0.995\}$
\end{itemize}

The average reward was computed over sliding windows of 100 episodes.

\subsection{Results with $\epsilon_{decay} = 0.999$}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{DQN999.png}
    \label{DQN}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Tabular999.png}
    \label{Tabular}
\end{figure}

\subsubsection{Deep Q-Network (DQN)}

The DQN agent initially exhibits very negative rewards due to high exploration (approximately $\epsilon \approx 0.9$ during early episodes). During the first 1000 episodes, performance improves steadily from approximately $-750$ to around $-27$. Around episode 2000, the average reward becomes positive, indicating that the agent has learned a reasonably effective policy. In the final phase (episodes 4000--6000), the performance stabilizes between 7 and 8 average reward, with small oscillations likely due to stochastic gradient updates and function approximation noise.


\subsubsection{Tabular Q-Learning}

The tabular agent follows a similar learning trajectory but shows slightly faster initial improvement. Performance becomes positive around episode 2100--2300. The convergence phase appears more stable compared to DQN.


\subsubsection{Comparative Analysis}

Both approaches reach comparable final performance levels. However, several differences can be observed:

\begin{itemize}
    \item \textbf{Convergence speed:} Tabular Q-learning improves slightly faster in early training, becoming positive around episodes 2100--2300 versus DQN's similar trajectory.
    \item \textbf{Stability:} Tabular learning exhibits lower variance during convergence, while DQN shows small oscillations due to stochastic optimization.
    \item \textbf{Final performance:} Both methods converge to similar average rewards ($\approx 7.2$--$7.7$), with DQN achieving 7.72 and Tabular achieving 7.19.
\end{itemize}

\subsubsection{Discussion}

The results are consistent with theoretical expectations. Taxi-v3 has a small, fully discrete state space (500 states). In such settings, tabular Q-learning can directly estimate the optimal action-value function $Q^*(s,a)$ without requiring function approximation. DQN approximates the Q-function using a neural network, which does not provide significant advantages for small discrete environments but may introduce additional variance due to stochastic optimization. The slower decay ($\epsilon_{decay}=0.999$) allows extensive exploration and produces stable long-run performance for both methods.

\subsection{Results with $\epsilon_{decay} = 0.997$}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{DQN997.png}
    \label{DQN}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Tabular997.png}
    \label{Tabular}
\end{figure}

\subsubsection{Deep Q-Network (DQN)}

With a faster exploration decay ($\epsilon_{decay}=0.997$), the agent reduces random exploration more quickly compared to the $\epsilon_{decay}=0.999$ setting. Early phase improvement is substantially faster: by episode 600 the average reward reaches approximately $-16.59$, and becomes positive already around episode 900 (Avg Reward $\approx 2.40$). The agent reaches the minimum exploration rate $\epsilon_{min}=0.01$ around episode 1600. During the mid-training phase (roughly episodes 1300--4500), the average reward remains mostly in the 7--8 range, indicating that an effective policy has been learned. However, the last part of training exhibits a noticeable degradation, culminating in a final average reward of approximately 5.34 at episode 6000. This late drop suggests instability typical of function approximation, where continued updates can slightly deteriorate a previously good policy.



\subsubsection{Tabular Q-Learning}

The tabular agent also benefits from the faster decay and exhibits rapid, stable learning progression. By episode 400 the average reward improves to approximately $-26.38$. The average reward becomes positive around episode 700. After reaching $\epsilon_{min}=0.01$ around episode 1600, performance stabilizes in the 7--8 range with relatively low variance. Unlike DQN, tabular Q-learning does not show a significant late-training collapse, with final reward remaining consistent with the plateau achieved during convergence.


\subsubsection{Comparative Analysis}

Both agents benefit from faster exploration reduction, but exhibit contrasting stability profiles:

\begin{itemize}
    \item \textbf{Convergence speed:} Both methods achieve positive rewards far earlier compared to $\epsilon_{decay}=0.999$ (around episodes 700--900 vs 2100--2400), with DQN reaching positive rewards by episode 900 and Tabular by episode 700.
    \item \textbf{Stability:} Tabular Q-learning remains stable throughout training and converges smoothly, while DQN exhibits late-training degradation with oscillations and drops.
    \item \textbf{Final performance:} Tabular achieves 7.57 versus DQN's 5.34, representing a significant difference of 2.23 in average reward at episode 6000.
\end{itemize}

\subsubsection{Discussion}

The faster decay ($\epsilon_{decay}=0.997$) accelerates learning early on, especially in Taxi-v3 where the state space is limited and exploration can be reduced sooner. However, it reveals a critical instability in DQN: when exploration becomes too small too early, replay buffers can become less diverse and the network may overfit to a narrow set of trajectories. Continued gradient updates can then degrade a previously good policy due to bootstrapping and approximation error accumulation. Tabular Q-learning, with its exact representation of $Q(s,a)$, remains robust and does not exhibit this instability, making it more reliable for aggressive exploration decay schedules.

\subsection{Results with $\epsilon_{decay} = 0.995$}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{DQN_0995_6000_Episodes.png}
    \label{DQN}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{Tabular_0995_6000_Episodes.png}
    \label{Tabular}
\end{figure}

\subsubsection{Deep Q-Network (DQN)}

Using a faster exploration decay ($\epsilon_{decay}=0.995$) strongly accelerates the transition from exploration to exploitation. The agent becomes positive very early: by episode 600 the average reward is already $\approx 2.60$. The minimum exploration rate ($\epsilon_{min}=0.01$) is reached around episode 1000, meaning that after this point the policy is almost purely greedy. After reaching the exploitation regime, the DQN performance remains mostly around 7--8 for a large portion of training, but it displays several significant drops (e.g., around episode 4000 and later). The final recorded average reward is approximately 5.04 at episode 6000, suggesting late-training instability.


\subsubsection{Tabular Q-Learning}

The tabular agent behaves differently under $\epsilon_{decay}=0.995$. Early learning is slower compared to DQN in this configuration (still negative at episode 1000 with Avg Reward $\approx -2.78$). After episode 1100 the average reward becomes positive and then steadily increases. The convergence phase is stable, with rewards consistently in the 7--8 range for most of the remaining training. Unlike DQN, tabular Q-learning maintains stable performance until the end of training, achieving a strong final average reward.


\subsubsection{Comparative Analysis}

The $\epsilon_{decay}=0.995$ setting reveals the most pronounced differences between the two approaches:

\begin{itemize}
    \item \textbf{Convergence speed:} DQN becomes positive much earlier (episode 600) than Tabular (episode 1100), despite Tabular entering the exploitation regime later, demonstrating the rapid initial learning of DQN with aggressive decay.
    \item \textbf{Stability:} Tabular maintains consistent 7--8 range performance throughout training, while DQN degrades significantly in the final phase with multiple performance drops.
    \item \textbf{Final performance:} Tabular achieves 7.34 versus DQN's 5.04, representing the largest performance gap of all tested decay schedules, with a difference of 2.30 in average reward.
\end{itemize}

\subsubsection{Discussion}

The most aggressive decay ($\epsilon_{decay}=0.995$) demonstrates a critical vulnerability in DQN: premature reduction of exploration combined with continuous stochastic gradient updates leads to performance collapse. When exploration is minimized around episode 1000, the replay buffer becomes dominated by a narrow range of experiences. Subsequent updates reinforce potentially suboptimal patterns, and the neural network architecture lacks the robust recovery mechanisms of tabular methods. Tabular Q-learning's exact value storage prevents this degradation: even with aggressive decay, the table maintains accurate estimates for all visited state-action pairs. This experiment highlights an important practical consideration: DQN requires more careful tuning of exploration schedules to maintain stability, whereas tabular methods are more forgiving to aggressive exploration decay in small discrete environments.

\section{Overall Comparative Analysis}

The comparison between DQN and Tabular Q-learning can be better understood by analyzing overall learning dynamics, convergence speed, stability, and sensitivity to exploration scheduling.

\subsection{Learning Dynamics}

Across all tested $\epsilon_{decay}$ values:

\begin{itemize}
    \item Faster decay rates (0.995, 0.997) accelerate early learning for DQN.
    \item Slower decay (0.999) delays convergence but produces smoother long-term behavior.
    \item Tabular Q-learning exhibits consistent and predictable improvement across all decay configurations.
\end{itemize}

\subsection{Stability}

The most significant difference emerges in training stability:

\begin{itemize}
    \item Tabular Q-learning shows low variance once convergence is reached.
    \item DQN displays higher oscillations and occasional late-stage degradation, especially under faster decay rates.
\end{itemize}

This suggests that DQN is more sensitive to exploration scheduling, while tabular learning remains robust.

\subsection{Performance Plateau}

When observing the performance plateau (approximately episodes 1500–5000):

\begin{itemize}
    \item Both methods consistently achieve average rewards in the 7–8 range.
    \item Tabular learning maintains this plateau more steadily.
    \item DQN often reaches the plateau earlier (with fast decay), but may fluctuate afterward.
\end{itemize}

\subsection{General Interpretation}

In Taxi-v3, a small and fully discrete environment:

\begin{itemize}
    \item Tabular Q-learning provides stable and reliable convergence across exploration schedules.
    \item DQN can achieve comparable performance but requires more careful tuning of $\epsilon_{decay}$ to avoid instability.
\end{itemize}

Overall, the trend analysis indicates that while DQN can learn quickly under aggressive decay settings, tabular Q-learning offers greater robustness and consistency in this domain.

\end{document}
